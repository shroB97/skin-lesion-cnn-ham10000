# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lys-JF8pynLKCEQrGFJpxCorC0Rujnq_
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

# -----------------------------
# PATHS
# -----------------------------
BASE_PATH = "/Users/shrobantibanerjee/downloads/archive"
IMG_DIR_1 = os.path.join(BASE_PATH, "HAM10000_images_part_1")
IMG_DIR_2 = os.path.join(BASE_PATH, "HAM10000_images_part_2")
METADATA_PATH = os.path.join(BASE_PATH, "HAM10000_metadata.csv")

# -----------------------------
# LOAD METADATA
# -----------------------------
def load_metadata():
    df = pd.read_csv(METADATA_PATH)
    le = LabelEncoder()
    df["dx_idx"] = le.fit_transform(df["dx"])
    return df, le

# -----------------------------
# TRAIN/VAL/TEST SPLIT
# -----------------------------
def split_data(df):
    train_df, test_df = train_test_split(df, test_size=0.15, stratify=df['dx_idx'], random_state=42)
    train_df, val_df = train_test_split(train_df, test_size=0.176, stratify=train_df['dx_idx'], random_state=42)
    return train_df, val_df, test_df

# -----------------------------
# CLASS WEIGHTS
# -----------------------------
def compute_class_weights(train_df):
    classes = np.unique(train_df["dx_idx"])
    weights = compute_class_weight("balanced", classes=classes, y=train_df["dx_idx"])
    class_weights = {i: w for i, w in enumerate(weights)}
    return class_weights

# -----------------------------
# IMAGE PATH MAPPER
# -----------------------------
def resolve_path(image_id):
    f1 = os.path.join(IMG_DIR_1, image_id + ".jpg")
    f2 = os.path.join(IMG_DIR_2, image_id + ".jpg")
    if os.path.exists(f1):
        return f1
    elif os.path.exists(f2):
        return f2
    else:
        raise FileNotFoundError(f"{image_id} not found!")

def add_paths(df):
    df["path"] = df["image_id"].apply(resolve_path)
    return df

# -----------------------------
# TF DATASET PIPELINE
# -----------------------------
IMG_SIZE = 224
BATCH_SIZE = 16

def load_image(path, label):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))
    img = tf.cast(img, tf.float32) / 255.0
    return img, label

def augment(img, label):
    img = tf.image.random_flip_left_right(img)
    img = tf.image.random_flip_up_down(img)
    img = tf.image.random_brightness(img, 0.2)
    img = tf.image.random_contrast(img, 0.8, 1.2)
    img = tf.image.random_rotation(img, 0.1)
    return img, label

def make_dataset(df, training=False):
    paths = df["path"].values
    labels = df["dx_idx"].values
    ds = tf.data.Dataset.from_tensor_slices((paths, labels))
    if training:
        ds = ds.shuffle(buffer_size=2000)
    ds = ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)
    if training:
        ds = ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)